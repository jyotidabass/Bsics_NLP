{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basics_NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNLep+7P0LdFFCtCYpd2IWm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyotidabass/Bsics_NLP/blob/main/Basics_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwIYCFJzCpvK"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "sentence = \"hi, how are you?\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeWRn0b0CyNc",
        "outputId": "0e49585c-a976-4512-aaa9-6b322463e4d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi,', 'how', 'are', 'you?']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "word_tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmRQ8mMiC4MO",
        "outputId": "fc7106c2-095b-4c7d-c057-48d1b1d8cdb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', ',', 'how', 'are', 'you', '?']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# create a corpus of sentences\n",
        "corpus = [\n",
        " \"hello, how are you?\",\n",
        " \"im getting bored at home. And you? What do you think?\",\n",
        " \"did you know about counts\",\n",
        " \"let's see if this works!\",\n",
        " \"YES!!!!\"\n",
        "]\n",
        "# initialize CountVectorizer\n",
        "ctv = CountVectorizer()\n",
        "# fit the vectorizer on corpus\n",
        "ctv.fit(corpus)\n",
        "corpus_transformed = ctv.transform(corpus)\n",
        "print(corpus_transformed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRoxkuThEVTm",
        "outputId": "3e2d2ab5-fa37-4725-a756-25952b3e475c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 2)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 11)\t1\n",
            "  (0, 22)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 3)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 8)\t1\n",
            "  (1, 10)\t1\n",
            "  (1, 13)\t1\n",
            "  (1, 17)\t1\n",
            "  (1, 19)\t1\n",
            "  (1, 22)\t2\n",
            "  (2, 0)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 6)\t1\n",
            "  (2, 14)\t1\n",
            "  (2, 22)\t1\n",
            "  (3, 12)\t1\n",
            "  (3, 15)\t1\n",
            "  (3, 16)\t1\n",
            "  (3, 18)\t1\n",
            "  (3, 20)\t1\n",
            "  (4, 21)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that index 22 belongs to “you” and in the second sentence, we have used\n",
        "“you” twice. Thus, the count is 2.The way CountVectorizer works is it first tokenizes the sentence and then assigns a\n",
        "value to each token. So, each token is represented by a unique index. These unique\n",
        "indices are the columns that we see. The CountVectorizer stores this information."
      ],
      "metadata": {
        "id": "Xx1bW2xUFcVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ctv.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uzYE9aAFPq6",
        "outputId": "1c3ed380-95a8-407e-c506-e7bdd890904c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hello': 9, 'how': 11, 'are': 2, 'you': 22, 'im': 13, 'getting': 8, 'bored': 4, 'at': 3, 'home': 10, 'and': 1, 'what': 19, 'do': 7, 'think': 17, 'did': 6, 'know': 14, 'about': 0, 'counts': 5, 'let': 15, 'see': 16, 'if': 12, 'this': 18, 'works': 20, 'yes': 21}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But\n",
        "we are missing some special characters. Sometimes those special characters can be\n",
        "useful too. For example, “?” denotes a question in most sentences. Let’s integrate\n",
        "word_tokenize from scikit-learn in CountVectorizer and see what happens."
      ],
      "metadata": {
        "id": "IcG138D5GNZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "# create a corpus of sentences\n",
        "corpus = [\n",
        " \"hello, how are you?\",\n",
        " \"im getting bored at home. And you? What do you think?\",\n",
        " \"did you know about counts\",\n",
        " \"let's see if this works!\",\n",
        " \"YES!!!!\"\n",
        "]\n",
        "# initialize CountVectorizer with word_tokenize from nltk\n",
        "# as the tokenizer\n",
        "ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "# fit the vectorizer on corpus\n",
        "ctv.fit(corpus)\n",
        "corpus_transformed = ctv.transform(corpus)\n",
        "print(ctv.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACHMeldeGR9I",
        "outputId": "f6d9a771-e715-42bf-b65b-1ce49da7de49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hello': 14, ',': 2, 'how': 16, 'are': 7, 'you': 27, '?': 4, 'im': 18, 'getting': 13, 'bored': 9, 'at': 8, 'home': 15, '.': 3, 'and': 6, 'what': 24, 'do': 12, 'think': 22, 'did': 11, 'know': 19, 'about': 5, 'counts': 10, 'let': 20, \"'s\": 1, 'see': 21, 'if': 17, 'this': 23, 'works': 25, '!': 0, 'yes': 26}\n"
          ]
        }
      ]
    }
  ]
}